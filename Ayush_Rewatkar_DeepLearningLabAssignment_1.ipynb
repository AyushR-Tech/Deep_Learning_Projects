{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice Lab Assignment 1**\n",
        "**Name:** Ayush Rewatkar    \n",
        "**Div:** A  \n",
        "**Batch:** DL-1  \n",
        "**Roll No:** 12  \n",
        "**PRN No:** 202201040033  "
      ],
      "metadata": {
        "id": "zahti8RVPz6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "7OAqdGfjOidh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ],
      "metadata": {
        "id": "51w22i3pOi_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load dataset**"
      ],
      "metadata": {
        "id": "q0tDjJAHOjVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Titanic dataset\n",
        "file_path = '/content/Titanic-Dataset.csv'  # Update this path if the file is stored elsewhere\n",
        "titanic_data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "RugpzzRsOjrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "51f-JruOOkPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values\n",
        "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].median())\n",
        "titanic_data['Embarked'] = titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0])"
      ],
      "metadata": {
        "id": "Y7L0aX8BOklP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One-Hot Encoding**"
      ],
      "metadata": {
        "id": "1OzgekvyOkz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical features\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Sex', 'Embarked'], drop_first=True)"
      ],
      "metadata": {
        "id": "AxBde-qkOlGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Features Selection**"
      ],
      "metadata": {
        "id": "1otB4RcuOlU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target\n",
        "encoded_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
        "target = 'Survived'\n",
        "\n",
        "# Extracting features (X) and target (y)\n",
        "X = titanic_data[encoded_features]\n",
        "y = titanic_data[target].values\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "HhkmxyEZOloQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split the Data into Training and Testing Sets**"
      ],
      "metadata": {
        "id": "7PD_GLsQOy9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Converting labels to one-hot encoded format\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
        "y_test_one_hot = encoder.transform(y_test.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "fqQwU1HZOzVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network Implementation**"
      ],
      "metadata": {
        "id": "jEubTt6gOznx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Parameters\n",
        "input_size = X_train.shape[1]  # 8 features\n",
        "hidden_size = 50  # 50 neurons in the hidden layer\n",
        "output_size = y_train_one_hot.shape[1]  # 2 output classes"
      ],
      "metadata": {
        "id": "x_CziDfyOz4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation Functions and Loss Function**"
      ],
      "metadata": {
        "id": "jWwgp0RtPpYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Add small epsilon for numerical stability\n",
        "\n",
        "# Forward Pass\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward Pass (Gradient Descent and Backpropagation)\n",
        "def backward(X, y, a1, a2):\n",
        "    m = X.shape[0]\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "    dz1 = np.dot(dz2, W2.T) * relu_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Training the model using gradient descent\n",
        "def train(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
        "    global W1, b1, W2, b2\n",
        "    for epoch in range(epochs):\n",
        "        a1, a2 = forward(X_train)\n",
        "        loss = cross_entropy_loss(a2, y_train)\n",
        "        dW1, db1, dW2, db2 = backward(X_train, y_train, a1, a2)\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "Pe4EljYDKvH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating the model**"
      ],
      "metadata": {
        "id": "Ez4AITIHO_TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "def predict(X):\n",
        "    _, a2 = forward(X)\n",
        "    return np.argmax(a2, axis=1)\n",
        "\n",
        "# Training the network\n",
        "train(X_train, y_train_one_hot, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_train = predict(X_train)\n",
        "y_pred_test = predict(X_test)\n",
        "\n",
        "y_true_train = np.argmax(y_train_one_hot, axis=1)\n",
        "y_true_test = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "train_accuracy = np.mean(y_pred_train == y_true_train) * 100\n",
        "test_accuracy = np.mean(y_pred_test == y_true_test) * 100\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR6gFB1cO_nQ",
        "outputId": "06b15b0f-8278-4869-d9ae-4eb0cf1c9533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 0.6934\n",
            "Epoch 100/1000, Loss: 0.5610\n",
            "Epoch 200/1000, Loss: 0.4396\n",
            "Epoch 300/1000, Loss: 0.4266\n",
            "Epoch 400/1000, Loss: 0.4172\n",
            "Epoch 500/1000, Loss: 0.4099\n",
            "Epoch 600/1000, Loss: 0.4037\n",
            "Epoch 700/1000, Loss: 0.3990\n",
            "Epoch 800/1000, Loss: 0.3946\n",
            "Epoch 900/1000, Loss: 0.3912\n",
            "Train Accuracy: 83.99%\n",
            "Test Accuracy: 81.56%\n"
          ]
        }
      ]
    }
  ]
}